# Video Pre-Processing Guide

This guide explains how to pre-render detection videos for optimal performance in the Axis-Guardian surveillance system.

## Overview

**Problem:** Real-time object detection (YOLO) is CPU-intensive and limits streaming performance to ~25-30 FPS.

**Solution:** Pre-render videos with baked-in detection boxes and metadata, enabling:
- ✅ **90-120 FPS** streaming (3-4x performance improvement)
- ✅ **70-80% CPU reduction** (no real-time YOLO inference)
- ✅ **Consistent, reproducible** detections for testing
- ✅ **Instant detection lookup** from JSON metadata
- ✅ **Minimal latency** with stream copy (no re-encoding)

## Quick Start

### 1. Pre-render All Videos

```bash
make prerender-videos
```

This **automatically detects and processes** all `.mp4` videos in `shared/cameras/` and generates:
- **Pre-rendered videos** with detection boxes: `shared/cameras/rendered/*.mp4`
- **Detection metadata** (JSON): `shared/cameras/rendered/*.detections.json`

**Smart Features:**
- ✅ Auto-detects all `.mp4` files in `shared/cameras/`
- ✅ Skips videos that are already pre-rendered
- ✅ Only re-renders if source video is newer than rendered version
- ✅ Shows processing summary (processed, skipped, failed)

### 2. Start the System

```bash
make dev
```

The system **automatically uses pre-rendered videos** if available, falling back to source videos otherwise.

### 3. Verify Performance

Check the streaming logs for:
```
✓ Using pre-rendered video (optimized)
✓ Loaded X detections across Y frames
```

## Detailed Workflow

### Directory Structure

```
shared/cameras/
├── people-detection.mp4                    # Source video
├── car-detection.mp4                       # Source video
├── person-bicycle-car-detection.mp4        # Source video
└── rendered/                                # Generated by pre-rendering
    ├── people-detection-rendered.mp4        # Pre-rendered with boxes
    ├── people-detection-rendered.detections.json  # Detection metadata
    ├── car-detection-rendered.mp4
    ├── car-detection-rendered.detections.json
    ├── person-bicycle-car-detection-rendered.mp4
    └── person-bicycle-car-detection-rendered.detections.json
```

### Pre-rendering Process

The pre-rendering script (`simulation/scripts/prerender_detections.py`) performs:

1. **Load source video** from `shared/cameras/`
2. **Run YOLO detection** on every frame (one-time processing)
3. **Draw bounding boxes** and labels directly on frames
4. **Save pre-rendered video** with H.264 encoding
5. **Export detection metadata** as timestamped JSON with **normalized bounding boxes**

**Output Files:**
- `{name}-rendered.mp4` - Video with baked-in detection boxes
- `{name}-rendered.detections.json` - Frame-by-frame detection data

### Detection Metadata Format

The JSON file contains normalized bounding boxes (0-1 range) compatible with the VAPIX standard:

```json
{
  "video_info": {
    "source_file": "people-detection.mp4",
    "width": 1920,
    "height": 1080,
    "fps": 30.0,
    "total_frames": 900
  },
  "detection_config": {
    "model": "yolov8n",
    "confidence_threshold": 0.5,
    "iou_threshold": 0.45
  },
  "frames": [
    {
      "frame_number": 0,
      "timestamp": 0.0,
      "detections": [
        {
          "bbox": {
            "left": 0.271,      // Normalized x1 (0-1 range)
            "top": 0.315,       // Normalized y1 (0-1 range)
            "right": 0.355,     // Normalized x2 (0-1 range)
            "bottom": 0.501     // Normalized y2 (0-1 range)
          },
          "confidence": 0.89,
          "class_id": 0,
          "class_name": "person",
          "timestamp": 0.0,
          "frame_number": 0
        }
      ]
    }
  ]
}
```

**Note:** Bounding boxes use **normalized coordinates only** (0-1 range), compliant with VAPIX standard. Pixel coordinates are not stored to avoid redundancy - they can be calculated as:
```javascript
x1 = bbox.left * video_width
y1 = bbox.top * video_height
x2 = bbox.right * video_width
y2 = bbox.bottom * video_height
```

## Commands Reference

### Pre-render All Videos (Auto-detect)

```bash
make prerender-videos
```

**What it does:**
- Automatically finds all `.mp4` files in `shared/cameras/`
- Skips videos that are already pre-rendered (unless source is newer)
- Processes only videos that need rendering
- Shows summary of processed, skipped, and failed videos

**Example output:**
```
Auto-detecting videos in shared/cameras/
Found 3 source videos
Skipping 1 videos (already rendered):
  ✓ car-detection.mp4
Processing 2 videos...
✓ Batch processing complete!
  Processed: 2
  Skipped: 1
  Failed: 0
```

### Force Re-render All Videos

```bash
make prerender-videos-force
```

Force re-rendering even if output files already exist. Useful when:
- Changing detection model or thresholds
- Detection quality issues
- Corrupted rendered videos

### Pre-render Single Video

```bash
make prerender-video VIDEO=people-detection.mp4
```

Process a specific video file.

### List Available Videos

```bash
make list-videos
```

Shows source videos and pre-rendered videos.

### Manual Invocation

```bash
# From project root
cd simulation/scripts

# Process all videos (auto-detect, skip already rendered)
../webrtc-detection/venv/bin/python prerender_detections.py --batch-all

# Force re-render all videos
../webrtc-detection/venv/bin/python prerender_detections.py --batch-all --force

# Process single video
../webrtc-detection/venv/bin/python prerender_detections.py \
  --input people-detection.mp4 \
  --output people-rendered.mp4

# Use custom model and thresholds
../webrtc-detection/venv/bin/python prerender_detections.py \
  --input video.mp4 \
  --model yolov8m.pt \
  --confidence 0.6 \
  --iou 0.5

# List available videos
../webrtc-detection/venv/bin/python prerender_detections.py --list

# Show help
../webrtc-detection/venv/bin/python prerender_detections.py --help
```

## Adding New Videos

### Step 1: Add Source Video

Place your video file in `shared/cameras/`:

```bash
cp ~/my-new-camera-video.mp4 shared/cameras/
```

### Step 2: Pre-render

**Option A: Let auto-detection handle it**
```bash
make prerender-videos  # Will automatically find and process your new video
```

**Option B: Process specific video**
```bash
make prerender-video VIDEO=my-new-camera-video.mp4
```

### Step 3: Configure Streaming

Update `simulation/scripts/stream-mock-cameras.sh`:

```bash
# Add to CAMERA_VIDEOS mapping
declare -A CAMERA_VIDEOS=(
    ["camera1"]="people-detection-rendered.mp4"
    ["camera2"]="car-detection-rendered.mp4"
    ["camera3"]="person-bicycle-car-detection-rendered.mp4"
    ["camera4"]="my-new-camera-video-rendered.mp4"  # Add this
)

# Add to SOURCE_VIDEOS mapping (fallback)
declare -A SOURCE_VIDEOS=(
    ["camera1"]="people-detection.mp4"
    ["camera2"]="car-detection.mp4"
    ["camera3"]="person-bicycle-car-detection.mp4"
    ["camera4"]="my-new-camera-video.mp4"  # Add this
)
```

### Step 4: Update WebRTC Config

Add camera to `simulation/webrtc-detection/src/config.py`:

```python
camera4_url: str = Field(
    default="rtsp://localhost:8554/camera4",
    description="Camera 4 RTSP URL"
)
```

And to `simulation/webrtc-detection/src/signaling.py`:

```python
camera_urls = {
    "camera1": settings.camera1_url,
    "camera2": settings.camera2_url,
    "camera3": settings.camera3_url,
    "camera4": settings.camera4_url,  # Add this
}
```

### Step 5: Update Pre-computed Detection Lookup

Add camera mapping in `simulation/webrtc-detection/src/video_track_precomputed.py`:

```python
camera_video_mapping = {
    'camera1': 'people-detection-rendered',
    'camera2': 'car-detection-rendered',
    'camera3': 'person-bicycle-car-detection-rendered',
    'camera4': 'my-new-camera-video-rendered',  # Add this
}
```

### Step 6: Test

```bash
make dev
```

Visit `http://localhost:5173/webrtc-detection` and verify your new camera appears.

## Performance Comparison

| Mode | FPS | CPU Usage | Detection Latency | Use Case |
|------|-----|-----------|-------------------|----------|
| **Real-time** | 25-30 | 80-90% | 50-100ms | Development, varying scenes |
| **Pre-rendered** | 90-120 | 10-20% | <5ms | Production, testing, demos |

## Troubleshooting

### Pre-rendered videos not being used

Check logs for:
```
⚠ Using source video (not optimized)
```

**Solution:**
1. Verify files exist: `ls shared/cameras/rendered/`
2. Run pre-rendering: `make prerender-videos`
3. Check file naming matches expectations

### Detection boxes not visible

**Issue:** Stream shows video but no boxes.

**Causes:**
1. Source video used instead of pre-rendered (see above)
2. Detection JSON file missing

**Solution:**
```bash
# Verify both files exist
ls shared/cameras/rendered/people-detection-rendered.mp4
ls shared/cameras/rendered/people-detection-rendered.detections.json

# Re-generate if missing
make prerender-video VIDEO=people-detection.mp4
```

### Out of memory during pre-rendering

**Issue:** Script crashes with OOM error.

**Solutions:**
1. **Use smaller inference size:**
   ```bash
   python prerender_detections.py --input video.mp4 --inference-size 320
   ```

2. **Process shorter videos** or split long videos into chunks

3. **Increase system swap space**

### Detection quality issues

**Issue:** Missing objects or false positives.

**Solutions:**
1. **Adjust confidence threshold:**
   ```bash
   python prerender_detections.py --input video.mp4 --confidence 0.3  # Lower = more detections
   ```

2. **Use larger YOLO model** (better accuracy, slower processing):
   ```bash
   python prerender_detections.py --input video.mp4 --model yolov8m.pt
   ```

3. **Check source video quality** (resolution, lighting, compression artifacts)

## Advanced Configuration

### Custom Detection Classes

Edit `simulation/scripts/prerender_detections.py`:

```python
# Add more class colors
CLASS_COLORS = {
    'person': (34, 197, 34),
    'car': (246, 130, 59),
    'dog': (255, 128, 0),      # Add custom classes
    'package': (0, 255, 255),
}
```

### Custom Video Encoding

Modify output settings in `prerender_detections.py`:

```python
# Example: Use higher quality encoding
fourcc = cv2.VideoWriter_fourcc(*'avc1')
out = cv2.VideoWriter(
    str(output_path),
    fourcc,
    fps,
    (width, height),
    params=[
        cv2.VIDEOWRITER_PROP_QUALITY, 95,  # Higher quality
    ]
)
```

### GPU Acceleration

The script automatically uses GPU if available:
- **CUDA (NVIDIA):** Automatically detected via PyTorch
- **CPU fallback:** Used if no GPU found

Check logs for:
```
Device: cuda   # GPU enabled
Device: cpu    # CPU fallback
```

## Integration with WebRTC Detection

The system uses a **hybrid approach**:

1. **Check for pre-rendered video:**
   - `shared/cameras/rendered/{camera}-rendered.mp4` exists?
   - Use stream copy (no re-encoding) → **Optimal performance**

2. **Check for detection JSON:**
   - `{video}.detections.json` exists?
   - Load detections into memory → **Instant lookup**

3. **Fallback to real-time:**
   - Source video + live YOLO detection → **Lower FPS**

This allows **seamless transition** between modes without code changes.

## Best Practices

1. **Always pre-render before production** - Use `make prerender-videos` as part of deployment
2. **Keep source videos** - Allows re-rendering with different models/thresholds
3. **Version control metadata** - Commit `.detections.json` files for reproducibility
4. **Monitor disk space** - Pre-rendered videos are ~same size as source
5. **Test performance gains** - Compare FPS before/after with browser dev tools

## File Size Expectations

| Video Type | Duration | Resolution | Source Size | Rendered Size |
|------------|----------|------------|-------------|---------------|
| Sample 1 | 30s | 1920x1080 | 5.2 MB | 5.8 MB |
| Sample 2 | 60s | 1280x720 | 3.1 MB | 3.5 MB |
| Sample 3 | 120s | 1920x1080 | 12.4 MB | 13.1 MB |

*Rendered videos are slightly larger due to drawn annotations, but encoding is optimized for streaming.*

## Further Reading

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [FFmpeg Streaming Guide](https://trac.ffmpeg.org/wiki/StreamingGuide)
- [WebRTC Best Practices](https://webrtc.org/getting-started/overview)
- [Axis VAPIX API](https://www.axis.com/vapix-library/)

## Support

For issues or questions:
1. Check logs: `simulation/mediamtx/mediamtx.log`
2. Verify setup: `make setup`
3. Test with source videos first
4. Open GitHub issue with logs and configuration
