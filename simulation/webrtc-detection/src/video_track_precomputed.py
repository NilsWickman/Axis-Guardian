"""
Pre-computed Detection Support Module

This module provides functionality to load and use pre-computed detections
from JSON files generated by the pre-rendering script.

Supports formats:
- JSON (.detections.json)
- JSON + gzip (.detections.json.gz) - recommended

Usage:
    from video_track_precomputed import load_precomputed_detections, get_detection_for_frame

    # Load detections (auto-detects format)
    detections_map = load_precomputed_detections("path/to/video.detections.json")

    # Get detections for specific frame
    frame_detections = get_detection_for_frame(detections_map, frame_number)
"""

import json
import gzip
from pathlib import Path
from typing import Dict, List, Any, Optional
from loguru import logger


def load_precomputed_detections(json_path: str) -> Optional[Dict[int, List[Any]]]:
    """
    Load pre-computed detections from file (auto-detects format).

    Supports:
    - JSON (.detections.json)
    - JSON + gzip (.detections.json.gz) - recommended

    Also supports legacy format (v1.0) and new format (v2.0) with sparse storage.

    Args:
        json_path: Path to detections file (JSON or JSON.gz)

    Returns:
        Dictionary mapping frame_number -> list of detections
        Returns None if file not found or loading fails
    """
    try:
        path = Path(json_path)

        # Try multiple file extensions if exact path doesn't exist
        paths_to_try = [path]
        if not path.exists():
            base = path.with_suffix('')
            paths_to_try = [
                base.with_suffix('.detections.json.gz'),  # Try compressed first
                base.with_suffix('.detections.json'),
            ]

        # Find first existing file
        actual_path = None
        for p in paths_to_try:
            if p.exists():
                actual_path = p
                break

        if actual_path is None:
            logger.warning(f"Pre-computed detections file not found: {json_path}")
            return None

        logger.info(f"Loading pre-computed detections from: {actual_path.name}")

        # Load based on file extension
        metadata = None
        if actual_path.suffix == '.gz' and '.json.gz' in actual_path.name:
            # JSON + gzip
            with gzip.open(actual_path, 'rt', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            # JSON (uncompressed)
            with open(actual_path, 'r') as f:
                metadata = json.load(f)

        if metadata is None:
            logger.error(f"Failed to parse detection file: {actual_path}")
            return None

        # Build frame_number -> detections mapping
        detections_map = {}
        frames_data = metadata.get('frames', [])

        # Check format version
        format_version = metadata.get('format_version', '1.0')

        for frame_data in frames_data:
            frame_num = frame_data['frame_number']
            detections = frame_data['detections']

            # Normalize detections (handle both v1.0 and v2.0 formats)
            normalized_detections = []
            for det in detections:
                # v2.0 format has nested bbox structure
                if 'bbox' in det and isinstance(det['bbox'], dict):
                    # Extract just the normalized coordinates for compatibility
                    normalized_det = {
                        'bbox': {
                            'left': det['bbox']['left'],
                            'top': det['bbox']['top'],
                            'right': det['bbox']['right'],
                            'bottom': det['bbox']['bottom'],
                        },
                        'confidence': det['confidence'],
                        'class_id': det['class_id'],
                        'class_name': det['class_name'],
                    }
                    # Optionally include detection_id if present
                    if 'detection_id' in det:
                        normalized_det['detection_id'] = det['detection_id']
                else:
                    # v1.0 format or already normalized
                    normalized_det = det

                normalized_detections.append(normalized_det)

            detections_map[frame_num] = normalized_detections

        # For sparse storage (v2.0), we need to handle total_frames from metadata
        total_frames_in_video = metadata.get('video_info', {}).get('total_frames', len(detections_map))
        total_frames_with_detections = len(detections_map)
        total_detections = sum(len(dets) for dets in detections_map.values())

        logger.info(
            f"✓ Loaded {total_detections} detections across {total_frames_with_detections} frames "
            f"(format v{format_version}, sparse: {total_frames_with_detections}/{total_frames_in_video} frames)"
        )

        # Log file size savings if using compressed format
        if actual_path.suffix == '.gz':
            uncompressed_size = sum(len(json.dumps(f).encode()) for f in frames_data)
            compressed_size = actual_path.stat().st_size
            logger.debug(f"Compression ratio: {100*compressed_size/uncompressed_size:.1f}% of original")

        return detections_map

    except Exception as e:
        logger.error(f"Failed to load pre-computed detections: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_detection_for_frame(
    detections_map: Dict[int, List[Any]],
    frame_number: int
) -> List[Any]:
    """
    Get detections for a specific frame.

    Args:
        detections_map: Dictionary from load_precomputed_detections()
        frame_number: Frame index to get detections for

    Returns:
        List of detection dictionaries for this frame (empty list if no detections)
    """
    if detections_map is None:
        return []

    # Handle looping videos: use modulo for frame lookup
    total_frames = max(detections_map.keys()) + 1 if detections_map else 1
    lookup_frame = frame_number % total_frames

    return detections_map.get(lookup_frame, [])


def find_detection_json_for_rtsp(rtsp_url: str, camera_id: str) -> Optional[str]:
    """
    Auto-detect the pre-computed detections JSON file for an RTSP stream.

    This function attempts to find the corresponding .detections.json file
    for a camera by looking in the rendered videos directory.

    Args:
        rtsp_url: RTSP stream URL (e.g., "rtsp://localhost:8554/camera1")
        camera_id: Camera identifier (e.g., "camera1")

    Returns:
        Path to detections JSON file if found, None otherwise
    """
    # Map camera IDs to expected rendered video names
    camera_video_mapping = {
        'camera1': 'people-detection-rendered',
        'camera2': 'car-detection-rendered',
        'camera3': 'person-bicycle-car-detection-rendered',
    }

    video_name = camera_video_mapping.get(camera_id)
    if not video_name:
        return None

    # Build path to detections JSON
    # Assumes structure: shared/cameras/rendered/{name}.detections.json
    project_root = Path(__file__).parent.parent.parent.parent
    detections_path = (
        project_root / "shared" / "cameras" / "rendered" /
        f"{video_name}.detections.json"
    )

    if detections_path.exists():
        logger.info(f"✓ Found pre-computed detections for {camera_id}")
        return str(detections_path)

    logger.debug(f"No pre-computed detections found for {camera_id} at {detections_path}")
    return None
